# FactTrace Hackathon @ University of Cambridge
## ðŸ§  Agentic AI Jury Hackathon
Welcome to the AI Jury Hackathon!

In this hackathon, youâ€™ll design a jury of AI agents that evaluates whether a statement faithfully represents a given fact or mutates it.
This is a fast, hands-on challenge focused on reasoning, disagreement, and trust, not infrastructure or large-scale systems.

## Contents
[Challenge](#challenge)\
[Judging Criteria](#-judging-criteria)\
[Important Question](#-important-question)\
[Possible Extension Direction](#-possible-extension-direction)\
[Getting Started](#-getting-started)\
[API Keys](#api-keys)

## Challenge
### ðŸš© The Core Problem
You will be given pairs of statements:
1. Internal Fact
A factual statement (e.g. from a report, paper, dataset, or trusted source)

2. External Interpretation
A statement derived from that fact (e.g. a summary, headline, tweet, or claim)

#### Your task is to answer:
> **Is the external claim a faithful representation of the internal fact â€” or a mutation?**

This is harder than it sounds. Mutations can happen in many ways, including but not limited to:

Most cases are borderline by design.
People can disagree on whether the meaning has changed.

#### What Counts as a â€œMutationâ€?
-Shifts in certainty or commitment
-Numerical rounding or threshold changes
-Scope or population drift
-Causality vs correlation confusion
-Changes in tone, strength, or implication
-Missing qualifiers or conditions

There is no single ground truth.
Your agentic jury must reason â€” not just classify.
---

### âš–ï¸ Build an AI Jury

You will design a **multi-agent system** - an *AI jury*.
A valid jury:
-Has multiple agents with distinct roles
-Allows agents to analyze, disagree, and refine

and it produces: 

- a verdict  
- an explanation
- a signal of uncertainty
  
ðŸš« A single LLM call in a loop is not sufficient.

This is not about being **â€œright at all costs.â€**  It is about reasoning **transparently and responsibly** to reach the verdict. 

---
### ðŸ Data Usage
You do not need to use all the data.

Instead, teams must:

1. Select a specific subset of the dataset
2. Justify why that subset is interesting or difficult
3. Apply multi-agent reasoning to it
   
Recommended subset size
- 10-20 claim pairs
  
Example valid subsets
- One domain (e.g. clinical, business, policy)
- One mutation type (e.g. numerical rounding, modality shift)
- One high-ambiguity subset (explicitly labeled in the CSV)
  
ðŸ“ The dataset is pre-categorized by:
- Domain
- Mutation type
- Ambiguity level
- High-ambiguity subset
  
Use these categories strategically.

---
### ðŸ Expected Output

By the end of the hackathon, your team should be able to:

- Demo a working AI jury built using multiple agents
- Show how agents reason, including where they agree or disagree
- Evaluate up to 5 (fact, claim) pairs, producing for each:
  - A final verdict
  - A concise, human-readable explanation
- Demonstrate that the approach generalizes beyond the presented examples
  (i.e., it is not hard-coded to specific cases)
- Explicitly communicate uncertainty whenever the evidence or agent reasoning is inconclusive


### Demos can be:
- CLI output  
- Notebook  
- Minimal UI  

> *Simple is great - clarity matters more than polish.*

##â±ï¸ The 5-Minute Demo Rule

Your solution should be understandable in 5 minutes or less.

A strong demo typically shows:

1. The chosen data subset
2. Agent roles and interaction
3. five concrete claim evaluations
4. Where agents agree, disagree, or defer
5. How does your system generalize
   
If it canâ€™t be explained in 5 minutes, itâ€™s probably too complex.
---

## ðŸ† Judging criteria 

Projects will be judged on:

#ðŸ§  Agent Design (30%)
- Clear, purposeful agent roles
- Meaningful interaction (not just chaining)
- Evidence of added value vs single-agent baseline
#ðŸ§¾ Reasoning & Explanation (30%)
- Clear, honest explanations
- Explicit handling of uncertainty
- Insight into why a case is hard
#ðŸ” Data Understanding (20%)
- Thoughtful subset selection
- Correct identification of mutation types
- Awareness of ambiguity
#ðŸŽ¤ Demo Clarity (20%)
- Clear structure
- Easy to follow
- Teaches the judges something

We care more about **good reasoning** than perfect answers.

### A golden solution is:
- Clearly better than a single-agent baseline  
- Understandable in a 3-minute demo  
- Teaches you something about multi-agent value  
- Handles uncertainty responsibly  

---

## ðŸ§  Important Question

During your demo, be prepared to answer:

> **What did using multiple agents add compared to a single AI call?**

There is no â€œcorrectâ€ answer â€” **thoughtful reflections are highly valued**.

---

## ðŸš€ Possible Extension Direction

Beyond defining agent roles, one way to extend your juryâ€™s capabilities is by giving agents access to **tools** â€” either by creating simple custom tools or by integrating existing ones.

If you choose to explore this path, you may find the **Model Context Protocol (MCP)** useful.

That said, tooling is optional. Extensions can take many forms, and you are encouraged to explore any idea that can improve your jury system.

---

## â±ï¸ Timebox & Mindset

This is a short, intense hackathon.

Aim for:

- A working prototype  
- One strong idea  
- A clear demo  

---

## ðŸš€ Getting Started

Ready to build? Here is everything you need to set up your environment, choose your tools, and manage your API usage.

---

### 1. The Repository

Everything you need to beginâ€”including the required datasets, skeleton files, and setup instructionsâ€”is in the official hackathon repository.

ðŸ‘‰ **Fork the Repository on GitHub**

All teams are expected to build on top of this repository to ensure a consistent baseline for evaluation.

---

### 2. Multi-Agent Frameworks

You are free to use any **Multi-Agent System (MAS)** framework you prefer, such as:

- AutoGen  
- CrewAI  
- LangGraph  

If you are new to building agents, we recommend **LangChain**.

**Why?**  
It makes composing agents quick and intuitive.

ðŸ“š **Docs:** [LangChain Documentation](https://docs.langchain.com)

---

## API Keys
> _(Loubna and James to review â€” feel free to change this part as necessary)_

To access the LLM services required for your agents, you will use your own API keys. We recommend **OpenAI** for its ease of use and documentation, though you are free to use **Anthropic**, **Gemini**, or others.

ðŸ’° **Reimbursement Policy**  
We will reimburse API costs incurred during the event up to **Â£[X] per team**.  
Please save your usage receipts or screenshots for submission at the end of the event.

ðŸ“‰ **Tips to keep costs low:**

- **Use efficient models:**  
  Start with cheaper models (e.g., `gpt-5-nano` or `gpt-5-mini`) for testing, and only switch to flagship models (e.g., `gpt-5`) for final, distinct tasks.
- **Limit your Jury:**  
  When testing, reduce the number of agents in your loop.
- **Watch the loop:**  
  Ensure your code has clear *exit conditions* to avoid infinite retry loops that drain credits.
- **Set hard limits:**  
  Configure a hard budget limit in your API providerâ€™s dashboard to prevent accidental overspending.

---

## ðŸŽ¯ Final Thought

Facts rarely break in obvious ways.  
They often break through **interpretation**.

Your job is to design an AI system that notices when that happens.

**Good luck â€” and have fun building your jury.**
